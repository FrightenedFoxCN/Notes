# 人工智能安全 AI Security

!!! info
    本文来源于 2023 年春杨子琪老师浙江大学的同名课程。

参考书：

- 《深度学习》 Ian Goodfellow et al
- 《对抗机器学习：机器学习系统中的攻击和防御》Tevgeniy Vorobeychik, Murat Kantarcioglu
- 《AI 安全之对抗样本入门》兜哥

## 人工智能基础

机器学习是人工智能的一个重要分支，而深度学习则是机器学习中的一个分支。在讨论人工智能安全时，我们关注的主要是深度学习的部分。下面先给出机器学习的工作流程：

- 明确问题：抽象待解决问题为机器学习的预测问题，明确分类任务（classification）还是回归任务（regression）；
- 数据选择：数据和特征作为机器学习结果的上限。数据的代表性、时间范围和业务范围；
- 特征工程：探索性数据分析、数据预处理、特征提取。将原始数据转化为模型可用的特征；
- 模型训练：训练集（training set），测试集（testing set），验证集（validation set）；
- 模型评估：欠拟合 / 过拟合；
- 模型决策：对预测信息进行分析解释，并应用于实际的工作领域；

表示学习（representation learning）考虑的是如何表示一个数据集合，对其进行结构化和索引。因此，直接让模型从数据中抽取特征或者表示，例如自编码器。它是机器学习的一个部分，而深度学习则是表示学习的一个分支。

在设计特征或尝试学习特征的过程中，我们的目标往往是分理处能解释观察数据的变差因素（factors of variation），它们往往需要以复杂的方式来进行辨识。深度学习尝试通过其它更加简单的表示来表达复杂的表示来解决表示学习中的核心问题。输入向可见层（visible layer）展示，而隐藏层（hidden layer）逐层从简单特征中组合出越来越复杂的特征。典型的例子是多层感知机（multilayer perceptron, MLP）。

注意，这里由于可解释性的不足，后门是可能的。哪怕在平均意义上它的准确性可能很高，但是必然存在一些边缘情形是它无法覆盖的。因此后面我们要思考的就是如何去构建和利用边缘情形，进而造就安全问题。

## 人工智能安全概览

人工智能技术的引入带来了一些新的安全挑战，例如数据泄露、感知欺骗攻击、AI 模型攻击等。根据不同的要素（数据、模型、运行环境）我们可以对其进行分类，如下：

- 数据方面：数据投毒攻击、模型逆向攻击、属性推断攻击；
- 模型方面：对抗样本攻击、模型窃取攻击、模型版权证明；
- 运行环境方面：服务器的安全威胁、客户端的安全威胁、网络的安全威胁；

前两者是本门课的重点，第三个方向则是传统信息安全的领域。另一种分类是从三个阶段划分：

- 设计阶段：数据偏见、模型偏见；
- 训练阶段：数据投毒、后门攻击；
- 使用阶段：对抗攻击、成员推断攻击；

第三种分类是系统架构分类：

- 硬件：电路扰动攻击、电磁和功耗数据泄露；
- 操作系统：不当用户权限、后门漏洞；
- AI 框架：依赖库的漏洞、AI 框架自身漏洞；
- 算法与数据：算法的偏见、算法的鲁棒性、算法的隐私性、数据的完整性；
- 模型：对抗攻击、后门攻击、成员推断攻击；

AI 安全同样具备与一般安全的 CIA 架构类似的形态：

- 保密性（confidentiality）：涉及的数据与模型信息不会泄露给没有授权的人；
- 完整性（integrity）：算法模型、数据、基础设施和产品不被恶意植入、篡改替换伪造；
- 鲁棒性（robustness）：能同时抵御复杂的环境条件和非正常的恶意干扰；
- 隐私性（secrecy）：在使用过程中能够保护数据主体的数据隐私；
- 公平性（fairness）：对不同的使用者，算法不会区别对待；

这是一个粗略的概括，实际上现在并没有统一的框架。

