# 自然语言处理 Natural Language Processing

!!! info
    本文来源于汤斯亮老师 2023 年春浙江大学的课程《自然语言处理导论》

## 深度学习基础 Deep Learning Basics

以监督学习的范式为例，它的目标就是拟合一个函数。例如，输入一个语音波形，输出转换成的文字；输入一张图片，输出其中物体的分类。这种映射一般可以看成两个域之间的转换，可以将其拆成两个函数的复合：编码和解码的过程。

在机器学习中，我们首先定义一族函数并称其为模型（model）。例如 $y = ax + b$ 是最简单的一族函数，寻找这一族函数中相对比较正确的映射关系就是在 $ab$-空间中进行选点和优化。为了度量这个函数是不是好的，一方面，我们需要一组训练数据，这是希望这个函数满足的目标；另一方面，需要给出一个评判标准来作为优化目标，这就是损失函数。这时事实上期望：选择出来的结果具备较好的泛化能力，即期待它在没有训练过的数据上具备良好的表现。这就需要测试（testing）或者推理（inference）的过程。

注意，对于泛化性只是一个良好的期望，在训练过程中，机器只需要去完成“记忆”就能取得好的评估结果，没有任何动力导致它做出泛化。总而言之，只需要三步：

1. 定义一族函数；
2. 定义评估标准；
3. 选择最优函数；

定义一族函数的时候，如果我们使用的是神经网络，那么它就被称为深度学习。

建模人的神经元连接是神经网络概念建立的初衷。神经元被认为是一族线性函数加上一族激活函数：

$$
\sigma(a_1w_1 + a_2w_2 + \cdots + a_nw_n + b)
$$

其中激活函数是一个阶跃函数，例如 Sigmoid 函数：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

这种函数也在数据挖掘中被用来做归一化和数据清洗，还有 $\tanh z$ 也是类似的用途，用来作为一个门控函数。这个模型中所有的 $w_i$ 和 $b$ 统称为参数（parameter）$\theta$。

最简单的神经网络就是全连接前馈神经网络（fully connect feedforward netword, FCN, FCFN，或称多层感知机，multilayer perceptron, MLP），其中每一层的每个神经元都与下一层的每个神经元相连接，层中的神经元数量可以任意设置。深度学习就是带有许多隐藏层的神经网络。例如 AlexNet(2012) 到 VGG(2014，最早的预训练模型)、GoogleNet(2014)。

注意到，Sigmoid 等激活函数对参数进行了压缩，因此，前传的结果可能更新受阻。因此产生了 ResNet(2015)，其中具备特殊的前馈结构，直接将上面数层的输出越级输入到另外一层，并去计算之间的差异，这样将其深度增加到了 152 层，其 top-5 错误率降至了 3.57%。

为什么深的神经网络比前的神经网络好呢？浅而宽的神经网络和深而窄的神经网络相比，由于近线性的激活函数 ReLU 的引入，可以认为其拟合的结果具备分段线性的结构。因此浅而宽的神经网络的分段比深而窄的神经网络相比分段数更少，深而窄的神经网络分段数多，但是计算量也随之增长。

考虑完选择一族函数的步骤之后，我们来考虑定义评估标准的过程。这就要求我们注意输入层和输出层的结构。输入层需要与数据表示想对应，而分类任务的输出层要额外通过 Softmax 层来进行一个“贫者愈贫，富者愈富”的过程。其中有一个参数温度（temperature）来增减这种效应。损失函数就是优化目标，是任务特定的。

最后是如何选择最佳函数的问题。梯度下降的方法是常规的，它要求有连续可导和凸性。随机梯度下降的方法是比较常用的，还有动量方法等等。我们希望通过这种方式可以找到一个比较好的局部极小值点。当然，梯度下降甚至可能不能找到局部极小值点，鞍点也会使得它停下。初始化的方法也是非常麻烦的。早期有一些方法会使用 RBM 来完成初始化过程，新的工作虽然不再需要这样，但也有一些关于初始化的研究。

深度神经网络的泛化能力不能用传统的学习理论来解释。传统的理论认为，只有在模型参数不足时它才具备学习泛化能力，典型的例如 VC 维理论。但是深度学习过程中，参数量是非常大的（overparametrized），这时神经网络会出现一些奇怪的现象：

1. 哪怕参数过多，它也能够发生泛化；
2. 在训练集上损失不再下降时，继续训练也能使得它具备更强的泛化能力。

有人主张这是因为神经网络自带正则化（regularization）的结构，因为过大的模型使得它的移动困难，或者通过损失函数施加了正则。Neural Tangent Kernel(2020) 通过对无限宽神经网络的研究给出了闭式解，并表明在参数无限条件下，深度神经网络会退化到 NTK 的形式。

关于神经网络结构的设计，从参数压缩（rate compression）的角度可以给出第一性原理（first principles, 2020），也就是说，优化目标就是找到一组降维方法，从而获得更高的压缩率。通过这种方法，可以推导出一些经典的神经网络结构。另外，关于 kernel machine(2020) 的工作指出，只要模型使用梯度下降的方法来进行优化，它实际上只是在对之前的样本进行记忆。当然，这里的核函数是一个路径核（path kernel），这才是真正学习到的东西。

## 词向量 Word Embedding

嵌入（embedding）是一个从低维流形往高维流形的连续单映。在这里，我们要用它来表征词。为了将词进行连续表示。最简单的方式就是独热表示（one-hot representation）。这种独热表示是一种相当简单的形式，通过人工构造特征来实现。一篇文章则被表征成所有词向量之和，可计算文章与文章之间的相似度。但问题在于，它是局域的。例如 New York Times 这个机构名，其中的三个词是相互连续的，这不会在词向量表征中被表达。另外，它的词与词之间是正交的，同义词和近义词不会被统一表征，例如 motel 和 hotel 在用户搜索时应当是近义的，但正交性使得它难以同样表达。

注意，词是一个高维空间，但意义是更加低位的结构。考虑将词表达在意义流形上做出一个降维，使得同义词和近义词的表达更加相近。这样，我们就达到了一个重要的观念：

You shall know a word by the company it keeps. (J. R. Firth, 1957)

这是现代统计自然语言处理中最成功的想法。考虑填空的形式，在相同上下文的前提下，如果两个词可交换，那么它们投影之后的结果应当是比较接近的，这就给出了一种降维的方向。降维之后的结果是一个实数空间，可以理解为语义空间。它往往是稠密的。这种观点在 2003 年由 Bengio 等人首创，但因为数据量太小导致效果不好。2013 年 Mikolov 等人的 word2vec 模型首次完成了高效的词向量嵌入。他们实现了连续词袋（continuous bag-of-word, CBOW）和 skip-gram(SG) 的操作，其模型输入了一个独热码，它是字典维度的；中间经过一个隐藏层（300 维左右）；最后在字典维度上输出以得到与之联系的结果。

> 考虑 eat an **apple** a day 这个短语。CBOW 是给出上下文要求其给出 apple 一词，SG 是给出 apple 一词让它预测上下文。

事实上，这个训练过程无非就是 $\sigma(W^\prime WX)$ 的求值过程。参见 [wevi (ronxin.github.io)](https://ronxin.github.io/wevi/) 中的可视化过程。最终得到的两个矩阵就给出了输入和输出的词向量（input vector 和 output vector）。其中的 input vector 事实上就是隐藏层的值，而 output vector 建模的是以这个词作为上下文的情况下给出的值，事实上和 $W^\prime$ 的伪逆一致。一般我们使用 input vector 作为词向量，但也有一些方法会使用 input vector 和 output vector 的平均值来作为词向量。

CBOW 和 SG 则是词向量使用的两种方式。CBOW 事实上是现在的预训练模型（如 BERT 等）的鼻祖，SG 则通常和负样本一起来完成，选取一对词来作为正样本和负样本，这和现在的对比学习（contrast learning）的思路是类似的。

另一种解释方式是通过 force-directed graph。考虑将每个词当成一个原子，中间由贡献度来决定拉力。一些优化如 hierarchical softmax 考虑加速输出的过程，使用类似 Hoffman 编码的形式来表征输出层，但这反而降低了计算的速度，因为它的并行度反而变低了。

词向量表征可以实现相似度预测、无监督机器翻译、词的类比等等，但它也有一些缺陷：

- 它只解决了近义词，没有解决一词多义的问题；
- 难以调试。通过这种自监督的方法得到的结果是很难调试的，例如，颜色词通常在同一语境下，但很多时候我们需要区分它们。
- 无序性，语序被视作无关的。
